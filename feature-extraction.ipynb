{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:15:50.790087Z",
     "start_time": "2024-01-22T10:15:50.215256Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import DenseNet121, ResNet50\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "target_size = (900, 400)\n",
    "\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize(target_size[::-1], Image.Resampling.LANCZOS)\n",
    "    # img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Normalize pixel values to between 0 and 1\n",
    "    return img_array\n",
    "\n",
    "\n",
    "input_person_image = load_and_preprocess_image('person_image.png')  # Replace with the actual path\n",
    "input_dress_image = load_and_preprocess_image('dress_image.png')  # Replace with the actual path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:07:55.830081Z",
     "start_time": "2024-01-22T10:07:55.663738Z"
    }
   },
   "id": "46ef79aa60f109f9"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'segmentation_mask'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Perform segmentation\u001B[39;00m\n\u001B[1;32m     17\u001B[0m input_image \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mexpand_dims(input_image, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m segmentation_result \u001B[38;5;241m=\u001B[39m \u001B[43msegmentation_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_image\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msegmentation_mask\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(segmentation_result\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Post-process the segmentation result\u001B[39;00m\n",
      "File \u001B[0;32m~/work/develop/sandbox/test-image-merge/venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m--> 153\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/work/develop/sandbox/test-image-merge/venv/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:961\u001B[0m, in \u001B[0;36m_check_index\u001B[0;34m(idx)\u001B[0m\n\u001B[1;32m    956\u001B[0m dtype \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(idx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _SUPPORTED_SLICE_DTYPES \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m    958\u001B[0m     idx\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(idx\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[1;32m    959\u001B[0m   \u001B[38;5;66;03m# TODO(slebedev): IndexError seems more appropriate here, but it\u001B[39;00m\n\u001B[1;32m    960\u001B[0m   \u001B[38;5;66;03m# will break `_slice_helper` contract.\u001B[39;00m\n\u001B[0;32m--> 961\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(_SLICE_TYPE_ERROR \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, got \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(idx))\n",
      "\u001B[0;31mTypeError\u001B[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'segmentation_mask'"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained segmentation model\n",
    "model_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "segmentation_model = hub.load(model_url)\n",
    "\n",
    "# Load and preprocess the input image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, (224, 224))  # Resize the image to match the model's input size\n",
    "    return img\n",
    "\n",
    "input_image_path = \"person_image.png\"\n",
    "input_image = load_and_preprocess_image(input_image_path)\n",
    "\n",
    "# Perform segmentation\n",
    "input_image = tf.expand_dims(input_image, axis=0)\n",
    "segmentation_result = segmentation_model(input_image)['segmentation_mask']\n",
    "print(segmentation_result.shape)\n",
    "\n",
    "# Post-process the segmentation result\n",
    "segmentation_result = tf.image.resize(segmentation_result, (input_image.shape[1], input_image.shape[2]))\n",
    "segmentation_result = tf.argmax(segmentation_result, axis=-1)\n",
    "segmentation_result = segmentation_result[0].numpy()\n",
    "\n",
    "# Create a mask for body parts\n",
    "body_parts_mask = np.where(segmentation_result == 15, 255, 0)  # Adjust the class ID according to your model's output\n",
    "\n",
    "# Apply the mask to the original image\n",
    "input_image = cv2.imread(input_image_path)\n",
    "segmented_image = cv2.bitwise_and(input_image, input_image, mask=body_parts_mask)\n",
    "\n",
    "# Display the segmented image\n",
    "cv2.imshow(\"Segmented Image\", segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:20:01.425479Z",
     "start_time": "2024-01-22T10:20:00.565787Z"
    }
   },
   "id": "7f2b111820789fc6"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Normalize and scale up the feature maps\n",
    "def process_features(features, original_size):\n",
    "    # Assuming we're using the first filter of the first layer\n",
    "    feature_map = features[0, :, :, 0]\n",
    "\n",
    "    # Normalize the feature map\n",
    "    feature_map -= feature_map.mean()\n",
    "    feature_map /= feature_map.std()\n",
    "    feature_map *= 64\n",
    "    feature_map += 128\n",
    "    feature_map = np.clip(feature_map, 0, 255).astype('uint8')\n",
    "\n",
    "    # Resize feature map to original size\n",
    "    feature_map_image = Image.fromarray(feature_map)\n",
    "    feature_map_image = feature_map_image.resize(original_size)\n",
    "    return feature_map_image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:07:56.598516Z",
     "start_time": "2024-01-22T10:07:56.590124Z"
    }
   },
   "id": "469406e8d7a6f6a0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 417ms/step\n",
      "(1, 29, 13, 2048)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/29x5vvy57135l3fqwbrn1l_80000gq/T/ipykernel_53818/3383339489.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  feature_map /= feature_map.std()\n",
      "/var/folders/3y/29x5vvy57135l3fqwbrn1l_80000gq/T/ipykernel_53818/3383339489.py:11: RuntimeWarning: invalid value encountered in cast\n",
      "  feature_map = np.clip(feature_map, 0, 255).astype('uint8')\n"
     ]
    }
   ],
   "source": [
    "person_input = Input(shape=(target_size[0], target_size[1], 3))\n",
    "# Load ResNet with pre-trained weights, excluding top layers\n",
    "resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=person_input)\n",
    "\n",
    "# Example: Adding a custom layer on top of ResNet\n",
    "# You would extend this for parsing and pose estimation branches\n",
    "x = resnet.output\n",
    "# ... Add custom layers ...\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=resnet.input, outputs=x)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "features_image_array = model.predict([input_person_image])\n",
    "print(features_image_array.shape)\n",
    "features_image_path = 'features_image.png'\n",
    "processed_feature_map = process_features(features_image_array, target_size)\n",
    "processed_feature_map.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:07:59.158925Z",
     "start_time": "2024-01-22T10:07:57.840817Z"
    }
   },
   "id": "371926bcf346e6f0"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1), <i8",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/work/develop/sandbox/test-image-merge/venv/lib/python3.9/site-packages/PIL/Image.py:3098\u001B[0m, in \u001B[0;36mfromarray\u001B[0;34m(obj, mode)\u001B[0m\n\u001B[1;32m   3097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3098\u001B[0m     mode, rawmode \u001B[38;5;241m=\u001B[39m \u001B[43m_fromarray_typemap\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtypekey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m   3099\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyError\u001B[0m: ((1, 1), '<i8')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Convert class probabilities to class labels\u001B[39;00m\n\u001B[1;32m      5\u001B[0m segmentation_map \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(segmentation_map, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m segmentation_map_resized \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43msegmentation_map\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mresize((target_size[\u001B[38;5;241m1\u001B[39m], target_size[\u001B[38;5;241m0\u001B[39m]), Image\u001B[38;5;241m.\u001B[39mResampling\u001B[38;5;241m.\u001B[39mNEAREST))\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Apply a colormap for visualization (optional)\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# The colormap helps differentiate the segments\u001B[39;00m\n\u001B[1;32m     10\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(segmentation_map_resized, cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mjet\u001B[39m\u001B[38;5;124m'\u001B[39m)  \u001B[38;5;66;03m# You can choose a different colormap as per your preference\u001B[39;00m\n",
      "File \u001B[0;32m~/work/develop/sandbox/test-image-merge/venv/lib/python3.9/site-packages/PIL/Image.py:3102\u001B[0m, in \u001B[0;36mfromarray\u001B[0;34m(obj, mode)\u001B[0m\n\u001B[1;32m   3100\u001B[0m         typekey_shape, typestr \u001B[38;5;241m=\u001B[39m typekey\n\u001B[1;32m   3101\u001B[0m         msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot handle this data type: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtypekey_shape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtypestr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 3102\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m   3103\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3104\u001B[0m     rawmode \u001B[38;5;241m=\u001B[39m mode\n",
      "\u001B[0;31mTypeError\u001B[0m: Cannot handle this data type: (1, 1), <i8"
     ]
    }
   ],
   "source": [
    "# Select the first image in the batch and remove the extra dimension\n",
    "segmentation_map = np.squeeze(features_image_array, axis=0)\n",
    "\n",
    "# Convert class probabilities to class labels\n",
    "segmentation_map = np.argmax(segmentation_map, axis=-1)\n",
    "segmentation_map_resized = np.array(Image.fromarray(segmentation_map).resize((target_size[1], target_size[0]), Image.Resampling.NEAREST))\n",
    "\n",
    "# Apply a colormap for visualization (optional)\n",
    "# The colormap helps differentiate the segments\n",
    "plt.imshow(segmentation_map_resized, cmap='jet')  # You can choose a different colormap as per your preference\n",
    "plt.colorbar()  # To show the color scale\n",
    "plt.axis('off')\n",
    "\n",
    "# Save or display the processed segmentation map\n",
    "plt.savefig('segmentation_output.png', bbox_inches='tight', pad_inches=0)\n",
    "# or display it\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T10:11:11.498826Z",
     "start_time": "2024-01-22T10:11:11.464264Z"
    }
   },
   "id": "995dc7689b2ebdb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c7c4e19a15debe79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
