{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c32ee3df0a24548",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:34:59.947058Z",
     "start_time": "2024-01-22T12:34:59.941319Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate, Conv2D, LeakyReLU, BatchNormalization, Flatten, Dense, UpSampling2D, Reshape\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import Adam\n",
    "from keras import activations\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a0395f73877a643",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:00.368383Z",
     "start_time": "2024-01-22T12:35:00.365620Z"
    }
   },
   "outputs": [],
   "source": [
    "target_size = (900, 400)\n",
    "head_x, head_y = 200, 350  # Replace with the actual coordinates\n",
    "\n",
    "# Normalize the coordinates\n",
    "normalized_head_x = head_x / target_size[1]\n",
    "normalized_head_y = head_y / target_size[0]\n",
    "\n",
    "# Use the normalized coordinates as input to the model\n",
    "sample_input_head_position = np.array([[normalized_head_x, normalized_head_y]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f76c61b4d6ac925",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:01.036658Z",
     "start_time": "2024-01-22T12:35:01.033691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize(target_size[::-1], Image.Resampling.LANCZOS)\n",
    "    # img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Normalize pixel values to between 0 and 1\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "aa20939a9a436e84",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:02.004960Z",
     "start_time": "2024-01-22T12:35:02.000870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create a segmentation mask (1 for foreground, 0 for background)\n",
    "def create_segmentation_mask(img_array):\n",
    "    # Assuming a simple segmentation mask based on pixel intensity.\n",
    "    mask = np.mean(img_array, axis=-1, keepdims=True) > 0.5\n",
    "    # mask = image.array_to_img(mask[0, :, :, 0])\n",
    "    # mask = mask.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    # mask = image.img_to_array(mask)\n",
    "    # mask /= 255.0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9bff27fccc662c69",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:02.580481Z",
     "start_time": "2024-01-22T12:35:02.435208Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load two input images\n",
    "input_person_image = load_and_preprocess_image('person_image.png')  # Replace with the actual path\n",
    "input_dress_image = load_and_preprocess_image('dress_image.png')  # Replace with the actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b563c5ebcbc1102",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:03.882717Z",
     "start_time": "2024-01-22T12:35:03.877758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 2)"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_head_position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "90524c40-97ea-4414-ae9e-ba1e01b49a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:04.081639Z",
     "start_time": "2024-01-22T12:35:04.079332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 900, 400, 3)"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dress_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6973157398532f88",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:04.499857Z",
     "start_time": "2024-01-22T12:35:04.475626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create segmentation masks for the person and dress images\n",
    "person_mask = create_segmentation_mask(input_person_image)\n",
    "dress_mask = create_segmentation_mask(input_dress_image)\n",
    "# Resize the masks to match the input image dimensions\n",
    "person_mask = np.array(Image.fromarray(person_mask[0, :, :, 0]).resize((target_size[1]*4, target_size[0]*4)))\n",
    "dress_mask = np.array(Image.fromarray(dress_mask[0, :, :, 0]).resize((target_size[1]*4, target_size[0]*4)))\n",
    "\n",
    "# Expand dimensions to match the input shape\n",
    "person_mask = np.expand_dims(person_mask, axis=-1)\n",
    "dress_mask = np.expand_dims(dress_mask, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "(3600, 1600, 1)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_mask.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:39:28.176585Z",
     "start_time": "2024-01-21T16:39:28.170868Z"
    }
   },
   "id": "ab8039f812b58726"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "# input_person = Input(shape=(target_size[0]*4, target_size[1]*4, 3), name='input_person')\n",
    "# input_dress = Input(shape=(target_size[0]*4, target_size[1]*4, 3), name='input_dress')\n",
    "# input_head_position = Input(shape=(2,), name='input_head_position')  # Example: (x, y) coordinates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:39:28.640514Z",
     "start_time": "2024-01-21T16:39:28.635075Z"
    }
   },
   "id": "e8a64c82baeb95fa"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "# Concatenate the person and dress images along the channels axis\n",
    "# concatenated_input = Concatenate(axis=-1)([input_person, input_dress])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:39:29.120321Z",
     "start_time": "2024-01-21T16:39:29.115241Z"
    }
   },
   "id": "94a7c568d58d0551"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    " # Add a Dense layer to incorporate head position information\n",
    "# head_position_embedding = Dense(64, activation='relu')(input_head_position)\n",
    "# head_position_embedding = Dense(64, activation='relu')(head_position_embedding)\n",
    "# head_position_embedding = Dense(128, activation='relu')(head_position_embedding)\n",
    "# expanded_head_position = Dense(target_size[0] * target_size[1], activation='relu')(input_head_position)\n",
    "# expanded_head_position = Reshape((target_size[0], target_size[1], 1))(expanded_head_position)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:39:29.403389Z",
     "start_time": "2024-01-21T16:39:29.395772Z"
    }
   },
   "id": "2385fd12158029b2"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# Concatenate the expanded head position with the concatenated input\n",
    "# concatenated_input_with_position = Concatenate(axis=-1)([concatenated_input, expanded_head_position])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:39:29.966544Z",
     "start_time": "2024-01-21T16:39:29.960492Z"
    }
   },
   "id": "2ef636ae41aeaf68"
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "422d4df68a1e5990",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:10.920328Z",
     "start_time": "2024-01-22T12:35:10.912241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generator network\n",
    "def build_generator():\n",
    "    input_person = Input(shape=(target_size[0], target_size[1], 3), name='input_person')\n",
    "    input_dress = Input(shape=(target_size[0], target_size[1], 3), name='input_dress')\n",
    "    input_head_position = Input(shape=(2,), name='input_head_position')  # Example: (x, y) coordinates\n",
    "    # Concatenate the person and dress images along the channels axis\n",
    "    concatenated_input = Concatenate(axis=-1)([input_person, input_dress])\n",
    "    print(concatenated_input.shape)\n",
    "    # Add a Dense layer to incorporate head position information\n",
    "    head_position_embedding = Dense(64, activation='relu')(input_head_position)\n",
    "    head_position_embedding = Dense(64, activation='relu')(head_position_embedding)\n",
    "    head_position_embedding = Dense(128, activation='relu')(head_position_embedding)\n",
    "    expanded_head_position = Dense(target_size[0] * target_size[1], activation='relu')(head_position_embedding)\n",
    "    expanded_head_position = Reshape((target_size[0], target_size[1], 1))(expanded_head_position)\n",
    "    # Concatenate the expanded head position with the concatenated input\n",
    "    concatenated_input_with_position = Concatenate(axis=-1)([concatenated_input, expanded_head_position])\n",
    "    gen = Conv2D(64, (3, 3), activation=activations.gelu, padding='same')(concatenated_input_with_position)\n",
    "    gen = UpSampling2D((2, 2))(gen)\n",
    "    gen = Conv2D(32, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(64, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(64, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(128, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(256, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(128, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(64, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = Conv2D(32, (3, 3), activation=activations.gelu, padding='same')(gen)\n",
    "    gen = UpSampling2D((2, 2))(gen)\n",
    "    output_img = Conv2D(3, (3, 3), activation=activations.sigmoid, padding='same')(gen)\n",
    "    # Apply segmentation masks to preserve the background\n",
    "    input_person_upsampled = UpSampling2D((4,4))(input_person)\n",
    "    input_dress_upsampled = UpSampling2D((4,4))(input_dress)\n",
    "    print(output_img.shape)\n",
    "    print(person_mask.shape)\n",
    "    print(input_person.shape)\n",
    "    output_img = output_img * (1 - person_mask) + input_person_upsampled * person_mask\n",
    "    output_img = output_img * (1 - dress_mask) + input_dress_upsampled * dress_mask\n",
    "    # Create the generator model\n",
    "    generator = Model(inputs=[input_person, input_dress, input_head_position], outputs=output_img)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def build_discriminator(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same')(input_img)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Add more layers as needed\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    validity = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    discriminator = Model(inputs=input_img, outputs=validity)\n",
    "    return discriminator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:12.309391Z",
     "start_time": "2024-01-22T12:35:12.300838Z"
    }
   },
   "id": "75776c385f8c91a1"
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "# Combined model (GAN)\n",
    "def build_gan(generator, discriminator):\n",
    "    # Disable training of the discriminator during GAN training\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    # Build GAN by chaining the generator and discriminator\n",
    "    input_person = Input(shape=(target_size[0], target_size[1], 3), name='input_person')\n",
    "    input_dress = Input(shape=(target_size[0], target_size[1], 3), name='input_dress')\n",
    "    input_head_position = Input(shape=(2,), name='input_head_position')\n",
    "    # input_head_position = Input(shape=(1,2), name='input_head_position')  # Example: (x, y) coordinates\n",
    "\n",
    "    # Generate an image using the generator\n",
    "    generated_img = generator([input_person, input_dress, input_head_position])\n",
    "\n",
    "    # Discriminator's decision on the generated image\n",
    "    validity = discriminator(generated_img)\n",
    "\n",
    "    # Combined GAN model\n",
    "    gan = Model(inputs=[input_person, input_dress, input_head_position], outputs=validity)\n",
    "    return gan"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:13.638182Z",
     "start_time": "2024-01-22T12:35:13.632143Z"
    }
   },
   "id": "581c7cb4b4a946ea"
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18eae0057b4322b7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:16.903955Z",
     "start_time": "2024-01-22T12:35:16.002117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 900, 400, 6)\n",
      "(None, 3600, 1600, 3)\n",
      "(3600, 1600, 1)\n",
      "(None, 900, 400, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Build the generator, discriminator, and GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator(input_shape=(target_size[0]*4, target_size[1]*4, 3))\n",
    "# Compile the discriminator model\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the generator model\n",
    "generator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(optimizer=Adam(learning_rate=0.0002, beta_1=0.5), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a160f12ba98bb5ec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:40:54.343218Z",
     "start_time": "2024-01-21T16:40:54.314417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_head_position (Input  [(None, 2)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " dense_46 (Dense)            (None, 64)                   192       ['input_head_position[0][0]'] \n",
      "                                                                                                  \n",
      " dense_47 (Dense)            (None, 64)                   4160      ['dense_46[0][0]']            \n",
      "                                                                                                  \n",
      " dense_48 (Dense)            (None, 128)                  8320      ['dense_47[0][0]']            \n",
      "                                                                                                  \n",
      " input_person (InputLayer)   [(None, 900, 400, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_dress (InputLayer)    [(None, 900, 400, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " dense_49 (Dense)            (None, 360000)               4644000   ['dense_48[0][0]']            \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " concatenate_19 (Concatenat  (None, 900, 400, 6)          0         ['input_person[0][0]',        \n",
      " e)                                                                  'input_dress[0][0]']         \n",
      "                                                                                                  \n",
      " reshape_9 (Reshape)         (None, 900, 400, 1)          0         ['dense_49[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_20 (Concatenat  (None, 900, 400, 7)          0         ['concatenate_19[0][0]',      \n",
      " e)                                                                  'reshape_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 900, 400, 64)         4096      ['concatenate_20[0][0]']      \n",
      "                                                                                                  \n",
      " up_sampling2d_4 (UpSamplin  (None, 1800, 800, 64)        0         ['conv2d_86[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 1800, 800, 32)        18464     ['up_sampling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 1800, 800, 64)        18496     ['conv2d_87[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 1800, 800, 64)        36928     ['conv2d_88[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 1800, 800, 128)       73856     ['conv2d_89[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 1800, 800, 256)       295168    ['conv2d_90[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 1800, 800, 128)       295040    ['conv2d_91[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 1800, 800, 64)        73792     ['conv2d_92[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 1800, 800, 32)        18464     ['conv2d_93[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d_5 (UpSamplin  (None, 3600, 1600, 32)       0         ['conv2d_94[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)          (None, 3600, 1600, 3)        867       ['up_sampling2d_5[0][0]']     \n",
      "                                                                                                  \n",
      " up_sampling2d_6 (UpSamplin  (None, 3600, 1600, 3)        0         ['input_person[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_40 (TFOpL  (None, 3600, 1600, 3)        0         ['conv2d_95[0][0]']           \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_41 (TFOpL  (None, 3600, 1600, 3)        0         ['up_sampling2d_6[0][0]']     \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_20 (T  (None, 3600, 1600, 3)        0         ['tf.math.multiply_40[0][0]', \n",
      " FOpLambda)                                                          'tf.math.multiply_41[0][0]'] \n",
      "                                                                                                  \n",
      " up_sampling2d_7 (UpSamplin  (None, 3600, 1600, 3)        0         ['input_dress[0][0]']         \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_42 (TFOpL  (None, 3600, 1600, 3)        0         ['tf.__operators__.add_20[0][0\n",
      " ambda)                                                             ]']                           \n",
      "                                                                                                  \n",
      " tf.math.multiply_43 (TFOpL  (None, 3600, 1600, 3)        0         ['up_sampling2d_7[0][0]']     \n",
      " ambda)                                                                                           \n",
      "                                                                                                  \n",
      " tf.__operators__.add_21 (T  (None, 3600, 1600, 3)        0         ['tf.math.multiply_42[0][0]', \n",
      " FOpLambda)                                                          'tf.math.multiply_43[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 47287843 (180.39 MB)\n",
      "Trainable params: 47287843 (180.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display the model summary\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "77b36b2ac15927b2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:27.641059Z",
     "start_time": "2024-01-22T12:35:20.429533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x345f64f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x345f64f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n"
     ]
    }
   ],
   "source": [
    "# Generate the merged image\n",
    "merged_image_array = generator.predict([input_person_image, input_dress_image, sample_input_head_position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "851db6c1c3b1205c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T12:35:29.494982Z",
     "start_time": "2024-01-22T12:35:29.196810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the merged image as a PNG file\n",
    "merged_image_path = 'merged_image.png'\n",
    "image.save_img(merged_image_path, merged_image_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 900, 400, 3)\n",
      "(320, 900, 400, 3)\n",
      "(1, 2)\n",
      "10/10 [==============================] - 79s 8s/step\n"
     ]
    }
   ],
   "source": [
    "person_images = np.full((1000, 900, 400, 3), input_person_image)\n",
    "dress_images = np.full((1000, 900, 400, 3), input_dress_image)\n",
    "# Set the number of training iterations\n",
    "num_iterations = 10\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 320\n",
    "\n",
    "# Set the update ratio (how many times to update the discriminator per generator update)\n",
    "update_ratio = 1\n",
    "\n",
    "# Training loop\n",
    "for iteration in range(num_iterations):\n",
    "    # ---------------------\n",
    "    # Train Discriminator\n",
    "    # ---------------------\n",
    "    for _ in range(update_ratio):\n",
    "        # Select a random batch of real person and dress images\n",
    "        idx = np.random.randint(0, person_images.shape[0], batch_size)\n",
    "        real_person_batch = person_images[idx]\n",
    "        real_dress_batch = dress_images[idx]\n",
    "        print(real_person_batch.shape)\n",
    "        print(real_dress_batch.shape)\n",
    "        print(input_head_position.shape)\n",
    "\n",
    "        # Generate a batch of fake images using the current generator\n",
    "        generated_images = generator.predict([real_person_batch, real_dress_batch])\n",
    "\n",
    "        # Create labels for real and fake samples\n",
    "        real_labels = np.ones((batch_size, 1))\n",
    "        fake_labels = np.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the discriminator on real samples\n",
    "        d_loss_real = discriminator.train_on_batch([generated_images], real_labels)\n",
    "\n",
    "        # Train the discriminator on fake samples\n",
    "        d_loss_fake = discriminator.train_on_batch([generated_images], fake_labels)\n",
    "\n",
    "        # Calculate total discriminator loss\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    # Train Generator\n",
    "    # ---------------------\n",
    "    # Select a new batch of real person and dress images\n",
    "    idx = np.random.randint(0, person_images.shape[0], batch_size)\n",
    "    real_person_batch = person_images[idx]\n",
    "    real_dress_batch = dress_images[idx]\n",
    "\n",
    "    # Create labels for the generator (tricking the discriminator)\n",
    "    valid_labels = np.ones((batch_size, 1))\n",
    "\n",
    "    # Train the generator to minimize the discriminator's loss\n",
    "    g_loss = gan.train_on_batch([real_person_batch, real_dress_batch], valid_labels)\n",
    "\n",
    "    # Print progress and losses (optional)\n",
    "    # if iteration % 100 == 0:\n",
    "    print(f\"Iteration {iteration}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-21T16:00:33.970432Z"
    }
   },
   "id": "1a4a99ce516c813f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4469031f-c6ab-4e44-bfee-3152d7b15aec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
