{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:49:09.013213Z",
     "start_time": "2024-01-21T16:49:09.005179Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPooling2D, concatenate\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "target_size = (900, 400)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:49:10.228701Z",
     "start_time": "2024-01-21T16:49:10.221653Z"
    }
   },
   "id": "701bed685eabde0b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize(target_size[::-1], Image.Resampling.LANCZOS)\n",
    "    # img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0  # Normalize pixel values to between 0 and 1\n",
    "    return img_array"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:49:11.641259Z",
     "start_time": "2024-01-21T16:49:11.636465Z"
    }
   },
   "id": "ae7af9a8b9b48cf2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "input_person_image = load_and_preprocess_image('person_image.png')  # Replace with the actual path\n",
    "input_dress_image = load_and_preprocess_image('dress_image.png')  # Replace with the actual path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:49:12.832089Z",
     "start_time": "2024-01-21T16:49:12.672046Z"
    }
   },
   "id": "246dd9a27f46cb15"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def create_network():\n",
    "    # Load a pre-trained VGG16 model for feature extraction\n",
    "    vgg = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Define two input layers\n",
    "    person_input = Input(shape=(target_size[0], target_size[1], 3))\n",
    "    clothing_input = Input(shape=(target_size[0], target_size[1], 3))\n",
    "\n",
    "    # Extract features using VGG16 for both inputs\n",
    "    person_features = vgg(person_input)\n",
    "    print(person_input.shape)\n",
    "    print(person_features.shape)\n",
    "    print(person_features)\n",
    "    clothing_features = vgg(clothing_input)\n",
    "\n",
    "    # Here, you would need a custom layer to combine these features\n",
    "    # This is a non-trivial task and would involve spatial transformations\n",
    "    # and possibly training a separate model to learn these transformations\n",
    "    combined_features = concatenate([person_features, clothing_features])\n",
    "\n",
    "    # Add some convolutional layers\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', strides=(1, 1))(combined_features)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', strides=(1, 1))(x)\n",
    "\n",
    "    # Output layer: output the same size as the input image\n",
    "    output = Conv2D(3, (3, 3), activation='sigmoid')(x)\n",
    "    print(output.shape)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[person_input, clothing_input], outputs=output)\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:56:45.632525Z",
     "start_time": "2024-01-21T16:56:45.626895Z"
    }
   },
   "id": "75d90742e45b0e75"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 900, 400, 3)\n",
      "(None, 28, 12, 512)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 28, 12, 512), dtype=tf.float32, name=None), name='vgg16/block5_pool/MaxPool:0', description=\"created by layer 'vgg16'\")\n",
      "(None, 12, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = create_network()\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:56:46.098797Z",
     "start_time": "2024-01-21T16:56:45.860583Z"
    }
   },
   "id": "510e8e6442b76388"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)       [(None, 900, 400, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " input_15 (InputLayer)       [(None, 900, 400, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " vgg16 (Functional)          (None, None, None, 512)      1471468   ['input_14[0][0]',            \n",
      "                                                          8          'input_15[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 28, 12, 1024)         0         ['vgg16[0][0]',               \n",
      " )                                                                   'vgg16[1][0]']               \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 28, 12, 64)           589888    ['concatenate_4[0][0]']       \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPoolin  (None, 14, 6, 64)            0         ['conv2d_12[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 14, 6, 128)           73856     ['max_pooling2d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 12, 4, 3)             3459      ['conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15381891 (58.68 MB)\n",
      "Trainable params: 15381891 (58.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:56:16.442749Z",
     "start_time": "2024-01-21T16:56:16.425174Z"
    }
   },
   "id": "f863fefcadd94a3a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 831ms/step\n"
     ]
    }
   ],
   "source": [
    "merged_image_array = model.predict([input_person_image, input_dress_image])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:49:27.583924Z",
     "start_time": "2024-01-21T16:49:26.715452Z"
    }
   },
   "id": "1c1982d711ae1bd"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Save the merged image as a PNG file\n",
    "merged_image_path = 'merged_image.png'\n",
    "image.save_img(merged_image_path, merged_image_array[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T16:49:29.054971Z",
     "start_time": "2024-01-21T16:49:29.041747Z"
    }
   },
   "id": "5ebd23eb5f8a0080"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fbf4e555b3d6d96c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
